{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd \n",
    "import wandb \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os, sys\n",
    "import wandb \n",
    "import time\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from causal_ccm.causal_ccm import ccm\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr\n",
    "# from nonlincausality.nonlincausality import nonlincausalityARIMA, nonlincausalityMLP\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "CAUSAL_PAIRS = [(\"predator_0\", \"predator_1\"), (\"predator_1\", \"predator_0\")]\n",
    "DIMENSIONS = [\"x\", \"y\", \"dx\", \"dy\", \"PCA_1\", \"PCA_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Entity: tpn\n",
      "    Project: wolf_sample\n",
      "    Number of runs: 364\n",
      "    Finished runs: 288\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# GLOBALS + wandb init\n",
    "ENTITY = 'tpn'\n",
    "PROJECT = 'wolf_sample'\n",
    "\n",
    "api = wandb.Api()\n",
    "num_runs = len(api.runs(path=f'{ENTITY}/{PROJECT}'))\n",
    "finished_runs = api.runs(\n",
    "    path=f'{ENTITY}/{PROJECT}', \n",
    "    filters={\"state\": \"finished\"})\n",
    "print(f\"\"\"\n",
    "    Entity: {ENTITY}\n",
    "    Project: {PROJECT}\n",
    "    Number of runs: {num_runs}\n",
    "    Finished runs: {len(finished_runs)}\n",
    "    \"\"\")\n",
    "runs_summary = pd.DataFrame()\n",
    "for run in finished_runs:\n",
    "    config = run.config\n",
    "    summary = run.summary\n",
    "    if run.state != 'finished':\n",
    "        print(f'Run {run.name} is not finished')\n",
    "        continue\n",
    "    time.sleep(0.01)\n",
    "    metrics = pd.DataFrame([dict(summary)])\n",
    "    metrics['algorithm_type'] = config['algorithm_type']\n",
    "    metrics['run_id'] = run.name\n",
    "    metrics['env_config'] = json.dumps(config['env_config'])\n",
    "    # metrics['nprey'] = config['env_config']['nprey']\n",
    "    # metrics['reward_lone'] = config['env_config']['reward_lone']\n",
    "    # metrics['reward_team'] = config['env_config']['reward_team']\n",
    "    metrics['config'] = json.dumps(config)\n",
    "    if len(runs_summary) == 0:\n",
    "        runs_summary = metrics\n",
    "    else:\n",
    "        runs_summary = pd.concat([runs_summary, metrics])\n",
    "\n",
    "# for a given set of nrpey and rteam, plot training curves\n",
    "# get new runs for the set of parameters \n",
    "def get_performance_df(nprey, rteam, algo):\n",
    "    collated_performances_df = None\n",
    "    struct_df = pd.DataFrame(\n",
    "            [],\n",
    "            index = np.arange(0, 50001, 500),\n",
    "            columns = ['episode_reward', 'episode_length', 'episode_assists', 'episodes_total']\n",
    "        ).fillna(0)\n",
    "    subset = api.runs(\n",
    "        path=f'{ENTITY}/{PROJECT}', \n",
    "        filters={\n",
    "            \"state\": \"finished\",\n",
    "            \"config.algorithm_type\": algo,\n",
    "            \"config.env_config.nprey\": nprey,\n",
    "            \"config.env_config.reward_team\": rteam\n",
    "            })\n",
    "    print(f\"\"\"\n",
    "            For nprey={nprey}, rteam={rteam}, algo={algo}:\n",
    "            {len(subset)} runs\n",
    "            \"\"\")\n",
    "    if len(subset) == 0:\n",
    "        return None \n",
    "    # get the training curves for the all runs in the subset \n",
    "    for run in subset:\n",
    "        run_name = run.name\n",
    "        performance_df = struct_df.copy()\n",
    "        history = pd.DataFrame(run._full_history())\n",
    "        df1 = history.loc[:, ['episode_reward_mean', 'episode_len_mean', 'episode_assists_mean', 'episodes_total']].dropna()\n",
    "        df1.columns = ['episode_reward', 'episode_length', 'episode_assists', 'episodes_total']\n",
    "        for col in df1.columns:\n",
    "            if col == 'epsides_total':\n",
    "                continue\n",
    "            performance_df[col] = np.interp(struct_df.index, df1['episodes_total'], df1[col])\n",
    "\n",
    "        collated_performances_df = np.dstack([np.array(struct_df), np.array(performance_df)])\\\n",
    "        if collated_performances_df is None else\\\n",
    "            np.dstack([collated_performances_df, np.array(performance_df)])\n",
    "    collated_performances_df = collated_performances_df[:, :, 1:]\n",
    "    collated_performances_df = pd.DataFrame(\n",
    "        collated_performances_df.mean(-1),\n",
    "        index = struct_df.index,\n",
    "        columns = struct_df.columns)\n",
    "    collated_performances_df = collated_performances_df.rolling(5, axis=0).mean().dropna()\n",
    "    return collated_performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES ANALYSIS\n",
    "def get_correlation(X, Y):\n",
    "    return [(\"correlation\", round(np.corrcoef(X, Y)[0][1],4))]\n",
    "\n",
    "def get_ccm(X, Y, lag):\n",
    "    E = lag\n",
    "    tau = 1\n",
    "    corr, pval = ccm(list(X), list(Y), tau=tau, E=E).causality()\n",
    "    if pd.isna(corr):\n",
    "        corr = 0.0\n",
    "        pval = -1.0\n",
    "    return [(\"ccm_score\", round(corr, 4)),  (\"ccm_pval\", round(pval, 4))]\n",
    "\n",
    "\n",
    "# Returns the results from fitting an OLS model to the data \n",
    "def get_granger_linear(X, Y, lag):\n",
    "    try:\n",
    "        data = np.array([X, Y]).T.astype(np.float64)\n",
    "        results = grangercausalitytests(data, maxlag=[lag], verbose=False)\n",
    "        ssr_ftest = results[lag][0][\"ssr_ftest\"]\n",
    "        chi_test = results[lag][0][\"ssr_chi2test\"]\n",
    "        return [\n",
    "        (\"F-statistic\", ssr_ftest[0]),\n",
    "        (\"F-p\", ssr_ftest[1]),\n",
    "        (\"chi2\", chi_test[0]),\n",
    "        (\"chi-p\", chi_test[1]),\n",
    "    ]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "nprey = 5\n",
    "algo_type = 'independent'\n",
    "rteam = 1.25\n",
    "\n",
    "def fetch_run_data(nprey, algo_type, rteam):\n",
    "\n",
    "    # get all the runs \n",
    "    subset = api.runs(\n",
    "        path=f'{ENTITY}/{PROJECT}', \n",
    "        filters={\n",
    "            \"state\": \"finished\",\n",
    "            \"config.algorithm_type\": algo_type,\n",
    "            \"config.env_config.nprey\": nprey,\n",
    "            \"config.env_config.reward_team\": rteam\n",
    "            })\n",
    "\n",
    "    if len(subset) == 0:\n",
    "        return None \n",
    "    \n",
    "    for run in subset:\n",
    "        history_df = None \n",
    "        performance_df = None \n",
    "        for file in run.files():\n",
    "            if \"history\" in file.name:\n",
    "                history_df = pd.read_json(\n",
    "                    file.download(exist_ok=True), orient='split'\n",
    "                )\n",
    "            if \"performance\" in file.name:\n",
    "                performance_df = pd.read_json(\n",
    "                    file.download(exist_ok=True), orient='split'\n",
    "                )\n",
    "        \n",
    "        if history_df is None:\n",
    "            continue \n",
    "            \n",
    "        print(len(history_df))\n",
    "        return history_df\n",
    "\n",
    "        analysis_results = perform_causal_analysis(\n",
    "            algo_type,\n",
    "            history_df,\n",
    "            CAUSAL_PAIRS,\n",
    "            DIMENSIONS,\n",
    "            L = 5000,\n",
    "            ccm_E = 4,\n",
    "            gc_lag = 4,\n",
    "            perform_ccm = True,\n",
    "        )\n",
    "        \n",
    "        print(analysis_results)\n",
    "\n",
    "\n",
    "def perform_causal_analysis(\n",
    "        policy_name,\n",
    "        observation_data,\n",
    "        causal_pairs, \n",
    "        dimensions,\n",
    "        L, \n",
    "        ccm_E,\n",
    "        gc_lag,\n",
    "        **kwargs):\n",
    "    # 1. For each fragment length, do causal analysis\n",
    "    analysis_results = []\n",
    "    for pair in causal_pairs:\n",
    "        for dim in dimensions:\n",
    "            X: pd.Series = observation_data.loc[:, f\"{pair[0]}_{dim}\"]\n",
    "            Y: pd.Series = observation_data.loc[:, f\"{pair[1]}_{dim}\"]\n",
    "\n",
    "            if len(X) <= L or len(Y) <= L:\n",
    "                raise Exception(\"Not Enough data\")\n",
    "            \n",
    "            X = X[:L]\n",
    "            Y = Y[:L]\n",
    "            \n",
    "            results = list()\n",
    "            results.extend(get_correlation(X, Y))\n",
    "            if kwargs.get('perform_ccm', False):\n",
    "                results.extend(get_ccm(X, Y, ccm_E))\n",
    "            \n",
    "            if kwargs.get('perform_granger_linear'):\n",
    "                results.extend(get_granger_linear(Y, X, gc_lag))\n",
    "                # results.extend(get_granger_arima(Y, X, gc_lag))\n",
    "\n",
    "            for result in results:\n",
    "                analysis_results.append(\n",
    "                        [\n",
    "                            (0, policy_name, *pair, result[0], dim),\n",
    "                            L,\n",
    "                            result[1],\n",
    "                        ]\n",
    "                    )\n",
    "                \n",
    "    return analysis_results\n",
    "\n",
    "history_df = fetch_run_data(nprey, algo_type, rteam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35096982568034873, 3.6128900248898666e-139)\n",
      "(0.4455174451598273, 7.855699557538382e-233)\n",
      "(0.08597333708624488, 2.4328276936826416e-09)\n",
      "(0.16122263105234122, 2.57834783375477e-29)\n",
      "(0.6639481197614152, 0.0)\n",
      "(0.6611454064003536, 0.0)\n",
      "(0.37386703786677733, 4.1740248603429686e-159)\n",
      "(0.35471525082922506, 2.55794928820229e-142)\n",
      "(-0.04100654135910868, 0.004490583692504956)\n",
      "(0.29040859109207856, 6.325464312406909e-94)\n",
      "(0.5753107458460549, 0.0)\n",
      "(0.7072192002144684, 0.0)\n"
     ]
    }
   ],
   "source": [
    "class spatial_ccm(ccm):\n",
    "    \"\"\"\n",
    "    We're checking causality X -> Y       \n",
    "    Args\n",
    "        X: timeseries for variable X that could cause Y\n",
    "        Y: timeseries for variable Y that could be caused by X\n",
    "        tau: time lag. default = 1\n",
    "        E: shadow manifold embedding dimension. default = 2\n",
    "        L: time period/duration to consider (longer = more data). default = length of X\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, sample_index, tau=1, E=2, L=None):\n",
    "        '''\n",
    "        X: timeseries for variable X that could cause Y\n",
    "        Y: timeseries for variable Y that could be caused by X\n",
    "        tau: time lag\n",
    "        E: shadow manifold embedding dimension\n",
    "        L: time period/duration to consider (longer = more data)\n",
    "        We're checking for X -> Y\n",
    "        '''\n",
    "        super().__init__(X, Y, tau, E, L)\n",
    "        self.My = self.dewdrop_shadow_manifold(Y, sample_index) # shadow manifold for Y (we want to know if info from X is in Y)\n",
    "        self.t_steps, self.dists = self.get_distances(self.My) # for distances between points in manifold\n",
    "\n",
    "    def dewdrop_shadow_manifold(self, V, sample_index):\n",
    "        \"\"\"\n",
    "        Given\n",
    "            V: some time series vector\n",
    "            tau: lag step\n",
    "            E: shadow manifold embedding dimension\n",
    "            L: max time step to consider - 1 (starts from 0)\n",
    "        Returns\n",
    "            {t:[t, t-tau, t-2*tau ... t-(E-1)*tau]} = Shadow attractor manifold, dictionary of vectors\n",
    "        \"\"\"\n",
    "        V = V[:self.L] # make sure we cut at L\n",
    "        sample_index = sample_index[:self.L]\n",
    "        grouped_df = pd.DataFrame({\"V\": V, \"sample_index\": sample_index}).groupby(\"sample_index\")\n",
    "\n",
    "        M = {t:[] for t in range((self.E-1) * self.tau, self.L)} # shadow manifold\n",
    "        for name, group in grouped_df:\n",
    "            L = len(group)\n",
    "            start_index = group.index[0]\n",
    "            for t in range((self.E-1) * self.tau, L):\n",
    "                v_lag = [] # lagged values\n",
    "                a = t + start_index\n",
    "                for t2 in range(0, self.E-1 + 1): # get lags, we add 1 to E-1 because we want to include E\n",
    "                    v_lag.append(V[a-t2*self.tau])\n",
    "                M[a] = v_lag\n",
    "            \n",
    "        filter_M = {k:v for k,v in M.items() if len(v) != 0} # filter out empty vectors\n",
    "        return filter_M\n",
    "    \n",
    "    # get pairwise distances between vectors in the time series\n",
    "    def get_distances(self, M):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            M: The shadow manifold from the time series\n",
    "        Returns\n",
    "            t_steps: timesteps\n",
    "            dists: n x n matrix showing distances of each vector at t_step (rows) from other vectors (columns)\n",
    "        \"\"\"\n",
    "\n",
    "        # we extract the time indices and vectors from the manifold M\n",
    "        # we just want to be safe and convert the dictionary to a tuple (time, vector)\n",
    "        # to preserve the time inds when we separate them\n",
    "        t_vec = [(k, v) for k,v in M.items() if len(v) != 0]\n",
    "        t_steps = np.array([i[0] for i in t_vec])\n",
    "        vecs = np.array([i[1] for i in t_vec])\n",
    "        dists = distance.cdist(vecs, vecs)\n",
    "        return t_steps, dists\n",
    "\n",
    "    def get_nearest_distances(self, t, t_steps, dists):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: timestep of vector whose nearest neighbors we want to compute\n",
    "            t_teps: time steps of all vectors in the manifold M, output of get_distances()\n",
    "            dists: distance matrix showing distance of each vector (row) from other vectors (columns). output of get_distances()\n",
    "            E: embedding dimension of shadow manifold M\n",
    "        Returns:\n",
    "            nearest_timesteps: array of timesteps of E+1 vectors that are nearest to vector at time t\n",
    "            nearest_distances: array of distances corresponding to vectors closest to vector at time t\n",
    "        \"\"\"\n",
    "        t_ind = np.where(t_steps == t) # get the index of time t\n",
    "        dist_t = dists[t_ind].squeeze() # distances from vector at time t (this is one row)\n",
    "\n",
    "        # get top closest vectors\n",
    "        nearest_inds = np.argsort(dist_t)[1:self.E+1 + 1] # get indices sorted, we exclude 0 which is distance from itself\n",
    "        nearest_timesteps = t_steps[nearest_inds] # index column-wise, t_steps are same column and row-wise\n",
    "        nearest_distances = dist_t[nearest_inds]\n",
    "\n",
    "        return nearest_timesteps, nearest_distances\n",
    "\n",
    "    def predict(self, t):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            t: timestep at manifold of y, My, to predict X at same time step\n",
    "        Returns\n",
    "            X_true: the true value of X at time t\n",
    "            X_hat: the predicted value of X at time t using the manifold My\n",
    "        \"\"\"\n",
    "        eps = 0.000001 # epsilon minimum distance possible\n",
    "        t_ind = np.where(self.t_steps == t) # get the index of time t\n",
    "        dist_t = self.dists[t_ind].squeeze() # distances from vector at time t (this is one row)\n",
    "        nearest_timesteps, nearest_distances = self.get_nearest_distances(t, self.t_steps, self.dists)\n",
    "\n",
    "        try:\n",
    "            # get weights\n",
    "            u = np.exp(-nearest_distances/np.max([eps, nearest_distances[0]])) # we divide by the closest distance to scale\n",
    "            w = u / np.sum(u)\n",
    "        except:\n",
    "            print(nearest_distances)\n",
    "            print(nearest_timesteps)\n",
    "            print(t)\n",
    "            print(self.t_steps)\n",
    "            print(self.dists)\n",
    "            raise\n",
    "        # get prediction of X\n",
    "        X_true = self.X[t] # get corresponding true X\n",
    "        X_cor = np.array(self.X)[nearest_timesteps] # get corresponding Y to cluster in Mx\n",
    "        X_hat = (w * X_cor).sum() # get X_hat\n",
    "\n",
    "    #     DEBUGGING\n",
    "    #     will need to check why nearest_distances become nan\n",
    "    #     if np.isnan(X_hat):\n",
    "    #         print(nearest_timesteps)\n",
    "    #         print(nearest_distances)\n",
    "\n",
    "        return X_true, X_hat\n",
    "\n",
    "# 1. For each fragment length, do causal analysis\n",
    "analysis_results = []\n",
    "for pair in CAUSAL_PAIRS:\n",
    "    for dim in DIMENSIONS:\n",
    "        X: pd.Series = history_df.loc[:, f\"{pair[0]}_{dim}\"]\n",
    "        Y: pd.Series = history_df.loc[:, f\"{pair[1]}_{dim}\"]\n",
    "        sample_index = history_df.loc[:, \"episode_num\"]\n",
    "\n",
    "        results = list()\n",
    "\n",
    "        # 1.1 Correlation\n",
    "        results.extend(get_correlation(X, Y))\n",
    "\n",
    "        # 1.2 CCM spatial \n",
    "        ccm_spatial = spatial_ccm(X, Y, sample_index, tau=1, E=5, L=5000).causality()\n",
    "        print(ccm_spatial)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

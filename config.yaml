algorithm_type: "independent"
algorithm_class: "ppo"
framework: "torch"
tune:
   tune: true # if true, tune hyperparameters in main!
   num_samples: 5
   max_episodes: 1000
   max_concurrent_trials: 8
analysis:
   save_eval_history: true 
   traj_length: 10000
   num_trials: 3
   analysis: true
   policy_set: ['original', 'follower', 'chaser']
   dimensions: ['x', 'y', 'PCA_1', 'PCA_2']
   length_fac: 200
   ccm_tau: 1
   ccm_E: 4
   gc_lag: 4
   perform_ccm: true
   perform_granger_linear: true 
training:
   lr: 0.0001
   use_critic: true
   use_kl_loss: true
   sgd_minibatch_size: 32
   num_sgd_iter: 2
   train_batch_size: 128
   grad_clip: "norm"
   gamma: 0.90
   model: 
      fcnet_hiddens: [128]
      fcnet_activation: "relu"
      conv_activation: "relu"
      conv_filters: [[16,[3,3],1], [16, [3,3], 1]]
      use_lstm: false
      lstm_cell_size: 64
      max_seq_len: 5
      lstm_use_prev_reward: true
      lstm_use_prev_action: true
      vf_share_layers: true
evaluate:
   eval_episodes: 500
rollouts:
   num_rollout_workers: 0
   batch_mode: "complete_episodes"
wandb: 
   wandb_init: true
   wandb_project: "wolf_sample"
   wandb_entity: "tpn"
   wandb_notes: "final training results"
   wandb_log_freq: 50
env_name: "wolfpack_discrete"
env_config:
   map_size: 20
   pred_vision: 3
   prey_type: "static"
   max_cycles: 100
   npred: 2
   nprey: 6
   reward_team: 1.0
   reward_lone: 1.0
   step_penalty: 0.0
ray:
   init_dashboard: false
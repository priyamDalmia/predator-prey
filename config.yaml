algorithm_type: "independent"
algorithm_class: "ppo"
framework: "torch"
tune:
   tune: false # if true, tune hyperparameters in main!
   num_samples: 20
   max_episodes: 25000
   max_concurrent_trials: 16
analysis:
   save_eval_history: True  
   traj_length: 50000
   num_trials: 3
   analysis: false 
   policy_set: ['original']
   dimensions: ['x', 'y', 'PCA_1', 'PCA_2', 'action']
   length_fac: 2000
   ccm_tau: 1
   ccm_E: 5
   gc_lag: 5
   perform_ccm: true
   perform_granger_linear: true 
training:
   lr: 0.0001
   use_critic: true
   use_kl_loss: true
   sgd_minibatch_size: 32
   num_sgd_iter: 2
   train_batch_size: 128
   grad_clip: "norm"
   gamma: 0.90
   model: 
      fcnet_hiddens: [128, 128]
      fcnet_activation: "relu"
      conv_activation: "relu"
      conv_filters: [[16,[3,3],1], [16, [3,3], 1]]
      use_lstm: false
      lstm_cell_size: 64
      max_seq_len: 5
      lstm_use_prev_reward: true
      lstm_use_prev_action: true
      vf_share_layers: true
evaluate:
   evaluation_duration: 1000
rollouts:
   num_rollout_workers: 0
   batch_mode: "complete_episodes"
wandb: 
   wandb_init: true 
   wandb_project: "project"
   wandb_entity: "tpn"
   wandb_notes: "final training results"
   wandb_log_freq: 50
env_name: "gather_v1"
env_config:
   map_size: 20
   pred_vision: 3
   prey_type: "static"
   max_cycles: 100
   npred: 2
   nprey: 6
   reward_team: 1.0
   reward_lone: 1.0
   step_penalty: 0.0
   pred_stun_rate: 10
ray:
   init_dashboard: false
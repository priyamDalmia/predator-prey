algorithm_type: "centralized"
algorithm_class: "ppo"
framework: "torch"
tune:
   tune: true # if true, tune hyperparameters in main!
   num_samples: 10
   max_episodes: 25000
   max_concurrent_trials: 24
analysis:
   num_trials: 2
   analysis: true
   policy_set: ['original', 'fixed', 'chaser']
   dimensions: ['dx', 'dy', 'PCA_1', 'PCA_2']
   length_fac: 500
   ccm_tau: 1
   ccm_E: 4
   pref_ccm_analysis: true
   pref_granger_analysis: false
   pref_graph_analysis: false
   pref_spatial_ccm_analysis: false
training:
   lr: 0.0001
   use_critic: true
   use_kl_loss: true
   sgd_minibatch_size: 32
   num_sgd_iter: 2
   train_batch_size: 128
   model: 
      fcnet_hiddens: [128]
      fcnet_activation: "relu"
      conv_activation: "relu"
      conv_filters: [[16,[3,3],1]]
      use_lstm: true
      lstm_cell_size: 128
      max_seq_len: 10
      lstm_use_prev_reward: true
      lstm_use_prev_action: true
      vf_share_layers: true
evaluate:
   eval_episodes: 500
rollouts:
   num_rollout_workers: 0
   batch_mode: "complete_episodes"
wandb: 
   wandb_init: true
   wandb_project: "rllib1"
   wandb_entity: "tpn"
   wandb_notes: "testing setup"
   wandb_log_freq: 50
env_name: "discrete_pp_v1"
env_config:
   map_size: 15
   pred_vision: 2
   prey_type: "static"
   max_cycles: 100
   npred: 2
   nprey: 6
   reward_type: "type_1"
   step_penalty: 0.0
ray:
   init_dashboard: false

Name of the cluster on which the job is executing:
	 spartan
Number of CPUs on the allocated node: 
	 1
Number of CPUs requested per task: 
	 1
Numer of GPUs requested: 
	 
Requested GPU count per allocated node: 
	 
Requested GPU count per allocated task:
	  
The ID of the job allocation:
	  53588002
Count of processors available to the job on this node:
	  1
Name of the job:
	  tst.slurm
List of nodes allocated to the job:
	  spartan-gpgpu079
Total number of nodes in the jobâ€™s resource allocation:
	  1
Name of the partition in which the job is running:
	  deeplearn
Minimum memory required per allocated CPU:
	  4000
Requested memory per allocated GPU:
	  
Total amount of memory per node that the job needs:
	  
List of nodes allocated to the job:
	  spartan-gpgpu079
Total number of CPUs allocated:
	  1
Maximum number of MPI tasks (thatâ€™s processes): 
	 1
Number of tasks requested per core: 
	 
Number of tasks requested per GPU: 
	 
Number of tasks requested per node:
	  1
The scheduling priority (nice value) at the time of job submission. This value is propagated to the spawned processes: 
	 0
The MPI rank (or relative process ID) of the current process: 
	 0
The directory from which SBATCH was invoked: 
	 /data/gpfs/projects/punim1355/dalmiapriyam/predator-prey
The Hostname of the computer from which SBATCH was invoked: 
	 spartan-login2.hpc.unimelb.edu.au
The process ID of the corresponding task: 
	 89813



 LOADING MODULES: 




 PYTHON SCRIPT OUTPUT: 

2023-11-19 17:41:55,470	INFO worker.py:1673 -- Started a local Ray instance.
CPU Affinity: [2]
[36m(hello_world pid=90114)[0m WORKER 3. Sleeping for (14) seconds.
Job done by 0!
Job done by 1!
Job done by 2!
Job done by 3!
Job done by 4!

Completed all tasks in 38.16 seconds.
[36m(hello_world pid=90116)[0m WORKER 1. Sleeping for (18) seconds.[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
Job ID           : 53588002
Cluster          : spartan
User/Project     : dalmiapriyam/punim1355
Nodes            : 1
Wall-clock time  : 00:00:42 / 00:05:00

Displaying overall resources usage from 2023-11-19 17:41:50 to 2023-11-19 17:42:32:

NODE            CPU#        TOT%   ( USR   / SYS   / WIO   / IDLE  ) 

spartan-gpgpu079 : 
                CPU# 1    : 14.5   (  11.2 /   3.3 /   0.0 /  85.5 ) 

                GPU# 1    : 0.0   


Allocated CPUs            : 1    
  CPUs with usage <25%    : 1    
  CPUs with usage <50%    : 0    
  CPUs with usage >50%    : 0    


Allocated GPUs            : 1    
  GPUs with usage <25%    : 1    
  GPUs with usage <50%    : 0    
  GPUs with usage >50%    : 0    

Memory used (RAM)         : 41.2%  [1728MB of 4195MB]

--------------------------------------------


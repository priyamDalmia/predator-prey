
Name of the cluster on which the job is executing:
	 spartan
Number of CPUs on the allocated node: 
	 1
Number of CPUs requested per task: 
	 1
Numer of GPUs requested: 
	 
Requested GPU count per allocated node: 
	 
Requested GPU count per allocated task:
	  
The ID of the job allocation:
	  53588025
Count of processors available to the job on this node:
	  1
Name of the job:
	  tst.slurm
List of nodes allocated to the job:
	  spartan-gpgpu079
Total number of nodes in the job‚Äôs resource allocation:
	  1
Name of the partition in which the job is running:
	  deeplearn
Minimum memory required per allocated CPU:
	  4000
Requested memory per allocated GPU:
	  
Total amount of memory per node that the job needs:
	  
List of nodes allocated to the job:
	  spartan-gpgpu079
Total number of CPUs allocated:
	  1
Maximum number of MPI tasks (that‚Äôs processes): 
	 1
Number of tasks requested per core: 
	 
Number of tasks requested per GPU: 
	 
Number of tasks requested per node:
	  1
The scheduling priority (nice value) at the time of job submission. This value is propagated to the spawned processes: 
	 0
The MPI rank (or relative process ID) of the current process: 
	 0
The directory from which SBATCH was invoked: 
	 /data/gpfs/projects/punim1355/dalmiapriyam/predator-prey
The Hostname of the computer from which SBATCH was invoked: 
	 spartan-login2.hpc.unimelb.edu.au
The process ID of the corresponding task: 
	 98191



 LOADING MODULES: 




 PYTHON SCRIPT OUTPUT: 

2023-11-19 17:44:27,069	WARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-19 17:44:30,756	INFO worker.py:1673 -- Started a local Ray instance.
2023-11-19 17:44:32,233	INFO tune.py:595 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
:job_id:01000000
:task_name:bundle_reservation_check_func
:actor_name:ImplicitFunc
:actor_name:train_algo
2023-11-19 17:44:33,095	WARNING util.py:62 -- Install gputil for GPU system monitoring.
2023-11-19 17:44:33,111	INFO wandb.py:307 -- Already logged into W&B.
wandb: Currently logged in as: theputernerdai (tpn). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/dalmiapriyam/ray_results/train_algo_2023-11-19_17-44-32/train_algo_186d1_00000_0_map_size=15,pred_vision=2,train_batch_size=200_2023-11-19_17-44-32/wandb/run-20231119_174434-186d1_00000
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train_algo_186d1_00000
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tpn/rllib2
wandb: üöÄ View run at https://wandb.ai/tpn/rllib2/runs/186d1_00000
slurmstepd: error: *** JOB 53588025 ON spartan-gpgpu079 CANCELLED AT 2023-11-19T17:49:44 DUE TO TIME LIMIT ***


Name of the cluster on which the job is executing:	 spartan
Number of CPUs on the allocated node: 	 12
Number of CPUs requested per task: 	 12
Numer of GPUs requested: 	 
Requested GPU count per allocated node: 	 
Requested GPU count per allocated task:	  
The ID of the job allocation:	  54116728
Count of processors available to the job on this node:	  12
Name of the job:	  tst.slurm
List of nodes allocated to the job:	  spartan-gpgpu079
Total number of nodes in the job’s resource allocation:	  1
Name of the partition in which the job is running:	  deeplearn
Minimum memory required per allocated CPU:	  4000
Requested memory per allocated GPU:	  
Total amount of memory per node that the job needs:	  
List of nodes allocated to the job:	  spartan-gpgpu079
Total number of CPUs allocated:	  1
Maximum number of MPI tasks (that’s processes): 	 1
Number of tasks requested per core: 	 
Number of tasks requested per GPU: 	 
Number of tasks requested per node:	  1
The scheduling priority (nice value) at the time of job submission. This value is propagated to the spawned processes: 	 0
The MPI rank (or relative process ID) of the current process: 	 0

The directory from which SBATCH was invoked: 	 /data/gpfs/projects/punim1355/dalmiapriyam/predator-prey
The Hostname of the computer from which SBATCH was invoked: 	 spartan-login3.hpc.unimelb.edu.au
The process ID of the corresponding task: 	 136412



 LOADING MODULES: 




 PYTHON SCRIPT OUTPUT: 

2023-12-05 14:24:36,499	WARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
Traceback (most recent call last):
  File "/data/gpfs/projects/punim1355/dalmiapriyam/predator-prey/./train.py", line 326, in <module>
    main()
  File "/data/gpfs/projects/punim1355/dalmiapriyam/predator-prey/./train.py", line 297, in main
    config['model']['use_lstm'] = tune.grid_search([True, False])
KeyError: 'model'
Job ID           : 54116728
Cluster          : spartan
User/Project     : dalmiapriyam/punim1355
Nodes            : 1
Wall-clock time  : 00:00:31 / 1-00:00:00

Displaying overall resources usage from 2023-12-05 14:24:17 to 2023-12-05 14:24:48:

NODE            CPU#        TOT%   ( USR   / SYS   / WIO   / IDLE  ) 

spartan-gpgpu079 : 
                CPU# 1    : 0.9    (   0.4 /   0.5 /   0.0 /  99.1 ) 
                CPU# 2    : 0.7    (   0.3 /   0.4 /   0.0 /  99.3 ) 
                CPU# 3    : 15.3   (  10.8 /   4.5 /   0.0 /  84.7 ) 
                CPU# 4    : 0.7    (   0.4 /   0.3 /   0.0 /  99.3 ) 
                CPU# 5    : 2.1    (   1.2 /   0.9 /   0.0 /  97.9 ) 
                CPU# 6    : 1.6    (   1.0 /   0.6 /   0.0 /  98.4 ) 
                CPU# 7    : 1.1    (   0.3 /   0.8 /   0.0 /  98.9 ) 
                CPU# 8    : 0.6    (   0.3 /   0.4 /   0.0 /  99.4 ) 
                CPU# 9    : 0.6    (   0.3 /   0.3 /   0.0 /  99.4 ) 
                CPU# 10   : 0.5    (   0.3 /   0.3 /   0.0 /  99.5 ) 
                CPU# 11   : 0.6    (   0.2 /   0.4 /   0.0 /  99.4 ) 
                CPU# 12   : 8.3    (   5.6 /   2.6 /   0.0 /  91.7 ) 

                GPU# 1    : 0.0   


Allocated CPUs            : 12   
  CPUs with usage <25%    : 12   
  CPUs with usage <50%    : 0    
  CPUs with usage >50%    : 0    


Allocated GPUs            : 1    
  GPUs with usage <25%    : 1    
  GPUs with usage <50%    : 0    
  GPUs with usage >50%    : 0    

Memory used (RAM)         : 0.0%  [4MB of 50332MB]

--------------------------------------------

